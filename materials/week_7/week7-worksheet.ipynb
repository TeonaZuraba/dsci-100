{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Lecture Worksheet - Classification continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Describe what a test data set is and how it is used in classification.\n",
    "* Using R, evaluate classification accuracy using a test data set and appropriate metrics.\n",
    "* Using R, execute cross validation in R to choose the number of neighbours.\n",
    "* Identify when it is necessary to scale variables before classification and do this using R\n",
    "* In a dataset with > 2 attributes, perform k-nearest neighbour classification in R using caret::train(method = \"knn\", ...) to predict the class of a test dataset.\n",
    "* Describe advantages and disadvantages of the k-nearest neighbour classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fruit Data Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the appropriate packages and \"fruit_data_with_colors.csv\" dataset into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "  library(tidyverse)\n",
    "  library(ggplot2)\n",
    "  library(forcats) # fct_recode()\n",
    "  library(readr)\n",
    "  library(caret)\n",
    "  library(readr)\n",
    "  library(repr)})  # change the size of the scatterplots \n",
    "fruitDat <- read_csv(\"data/fruit_data_with_colors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Let's take a look at the first six observations in the fruit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide this cell\n",
    "head(fruitDat)\n",
    "\n",
    "fruitDat <- fruitDat %>% \n",
    "  mutate(fruit_name = as.factor(`fruit_name`)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Find the nearest neighbour based on mass and width to the first observation just by looking at the scatterplot below (the first observation has been circled for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide cell\n",
    "options(repr.plot.width=6, repr.plot.height=4)\n",
    "point <- c(192, 8.4)\n",
    "fruitDat %>%  \n",
    "  ggplot(aes(x=mass, y= width, color = fruit_name)) + \n",
    "  scale_x_continuous(name = \"Mass (grams)\") +\n",
    "  scale_y_continuous(name = \"Width (cm)\") +\n",
    "  geom_point() +\n",
    "    annotate(\"path\", x=point[1] + 10*cos(seq(0,2*pi,length.out=100)),\n",
    "                   y=point[2]+0.2*sin(seq(0,2*pi,length.out=100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using mass and width, calculate the distance between the first observation and the second observation (coordinates $(180, 8.0)$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANSWER\n",
    "filter(fruitDat, row_number() %in% c(1, 2)) %>% # we filter the fruit dataset to pick out rows 1 and 2\n",
    "    select(mass, width) %>%  # select the columns we want from the dataset\n",
    "    dist()                   # calculate the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Calculate the distance between the first and the the 44th observation in the fruit dataset using the mass and width variables. We see from the table below, observation 44 has mass = 194 g and width = 7.2 cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(fruitDat, row_number() == 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANSWER \n",
    "filter(fruitDat, row_number() %in% c(1, 44)) %>%\n",
    "    select(mass, width) %>%\n",
    "    dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) <!-- better as a discussion question? --> \n",
    "i) What do you notice about your answers from a) and b) that you just calculated? (Hint: look at where the observations are on the scatterplot)\n",
    "\n",
    "ii) Is it what you would expect? Why or why not? (Hint: what might happen if we changed grams into kilograms to measure the mass?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide cell\n",
    "options(repr.plot.width=6, repr.plot.height=4)\n",
    "point1 <- c(192, 8.4)\n",
    "point2 <- c(180, 8)\n",
    "point44 <- c(194, 7.2)\n",
    "fruitDat %>%  \n",
    "  ggplot(aes(x=mass, y= width, color = fruit_name)) + \n",
    "  scale_x_continuous(name = \"Mass (grams)\") +\n",
    "  scale_y_continuous(name = \"Width (cm)\") +\n",
    "  geom_point() +\n",
    "    annotate(\"path\", x=point1[1] + 5*cos(seq(0,2*pi,length.out=100)),\n",
    "                   y=point1[2]+0.1*sin(seq(0,2*pi,length.out=100))) +\n",
    " annotate(\"path\", x=point2[1] + 5*cos(seq(0,2*pi,length.out=100)),\n",
    "                   y=point2[2]+0.1*sin(seq(0,2*pi,length.out=100))) +\n",
    "    annotate(\"path\", x=point44[1] + 5*cos(seq(0,2*pi,length.out=100)),\n",
    "                   y=point44[2]+0.1*sin(seq(0,2*pi,length.out=100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: \n",
    "The distance between the first and second observation is 12.01 and the distance between the first and 44th observation is 2.33. So by the formula, observation 1 and 44 are closer. However, if we look at the scatterplot the distance of the first observation to the second observation appears closer than to the 44th observation. \n",
    "\n",
    "Because the classifier predicts class by identifying the nearest points, the scale of the variables matters. Variables on a large scale compared to variables a small scale will have a greater effect on the distance between the observations. Here we have width (measured in cm) and mass (in grams). As far as knn is concerned, a difference of 12 g in mass between observation 1 and 2 is large compared to a difference of 1.2 cm in width between observation 1 and 44. Consequently, mass will drive the classification results, and width will have less of an effect. Hence, our distance calculation reflects that. Also, if we measured mass in kilograms, or if we measured width in meters, then weâ€™d get different classification results. Thus we can standardize the data so that all variables will be on a comparable scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) i) Scale all the variables of the fruit dataset and save them as columns in your data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "fruitDat <- fruitDat %>%\n",
    "  mutate(scaled_mass = scale(mass),\n",
    "         scaled_width = scale(width),\n",
    "         scaled_height = scale(height), \n",
    "         scaled_color_score = scale(color_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Let's repeat Q1 parts b) and c) with the scaled variables. Calculate the distance with the scaled mass and width variables between observation 1 and 2.  Calculate the distances with the scaled mass and width variables between observation 1 and 44. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "fruitDat %>% \n",
    "  select(scaled_mass, scaled_width) %>% \n",
    "  filter(row_number() %in% c(1,2)) %>%\n",
    "  dist()\n",
    "\n",
    "fruitDat %>% \n",
    "  select(scaled_mass, scaled_width) %>% \n",
    "  filter(row_number() %in% c(1,44)) %>%\n",
    "  dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2)i) Partition the data into a training (70%) and testing (30%) set using the `caret` package. Select the color score, mass, and the fruit name variables to include in your sets. Set seed to 6 so consistent results are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### R caret \n",
    "set.seed(6)      # set seed so same results can be obtained \n",
    "\n",
    "# create a vector of fruit name labels \n",
    "labels <- fruitDat %>% \n",
    "  pull(fruit_name)   # pull out a vector rather than having a dataframe\n",
    "\n",
    "# randomly take 70% of the data in the training set proportional to the different number of fruit names in the dataset\n",
    "# list = F denotes that the indices we obtain should form a vector\n",
    "\n",
    "# create index to split based on labels\n",
    "index <- createDataPartition(labels, p = 0.7, list = F)\n",
    "\n",
    "# filtering the dataset into training data based on the random indices above\n",
    "# selecting the scaled columns \"mass\" and \"color score\"\n",
    "trainingDat <- fruitDat %>% \n",
    "    select(scaled_mass, scaled_color_score) %>% \n",
    "    filter(row_number() %in% index)\n",
    "\n",
    "# create a vector of labels to use with knn by extracting the column \"fruit_name\" for your training set observations\n",
    "training_labels <- fruitDat %>% \n",
    "    select(fruit_name) %>% \n",
    "    filter(row_number() %in% index) %>%\n",
    "    pull()\n",
    "\n",
    "# creating the testing set based on the remaining observations \n",
    "# selecting the scaled columns \"mass\" and \"color score\" \n",
    "testingDat <- fruitDat %>% \n",
    "  select(scaled_mass, scaled_color_score) %>% \n",
    "  filter(!(row_number() %in% index))\n",
    "\n",
    "# create a vector of labels to use with knn by extracting the column \"fruit_name\" for your testing set observations\n",
    "testing_labels <- fruitDat %>% \n",
    "  select(fruit_name) %>% \n",
    "  filter(!(row_number() %in% index)) %>% \n",
    "    pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Using the `caret` package, perform knn to obtain fruit name predictions using \"color_score\" and \"mass\" variables. Use $k = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe with k = 5\n",
    "k5 <- data.frame(k = 5)\n",
    "\n",
    "# set the \"x\" argument equal to your training data as a data.frame \n",
    "# set the \"y\" argument equal to the training labels vector \n",
    "# set \"tuneGrid\" to your \"k\" dataframe\n",
    "# set the \"trControl\" argument equal to trainControl(method = \"none\") - we will talk more about this argument later\n",
    "model_knn5 <- train(x = data.frame(trainingDat),     \n",
    "                   y = training_labels,\n",
    "                   method = \"knn\", \n",
    "                   tuneGrid = k5,                               \n",
    "                   trControl = trainControl(method = \"none\"))  \n",
    "\n",
    "(predictions <- predict(object=model_knn5, testingDat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Choose one case that was not predicted correctly. What was predicted, and what is the correct label? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function \"cbind()\" to compare your actual and predicted values \n",
    "cbind(testingDat, testing_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Evaluate the classification performance by comparing the estimated labels to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a confusion matrix of the actual vs. predicted values \n",
    "confusionMatrix(predictions, testing_labels) \n",
    "\n",
    "# use the function \"table()\" to make a table of actual vs. predicted values \n",
    "table(predictions, testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv) Compute the overall accuracy of the knn classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the overall accuracy of the knn learner using the mean() function.\n",
    "mean(predictions == testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 2 labels were misclassified. Accuracy = $\\frac{12}{15} = 0.8667$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v) Compare $k$ values of 1, 5 and 15 to examine the impact on classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the k = 1 model using your code above\n",
    "k1 <- data.frame(k = 1)\n",
    "\n",
    "# set the \"x\" argument equal to your training data as a data.frame \n",
    "# set the \"y\" argument equal to the training labels vector \n",
    "# set \"tuneGrid\" to your \"k\" dataframe\n",
    "# set the \"trControl\" argument equal to trainControl(method = \"none\") - we will talk more about this argument later\n",
    "model_knn1 <- train(x = data.frame(trainingDat),     \n",
    "                   y = training_labels,\n",
    "                   method = \"knn\", \n",
    "                   tuneGrid = k1,                               \n",
    "                   trControl = trainControl(method = \"none\"))  \n",
    "\n",
    "predictions1 <- predict(object=model_knn1, testingDat)\n",
    "mean(predictions1 == testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the \"train\" function by setting k = 15\n",
    "k15 <- data.frame(k = 15)\n",
    "\n",
    "# set the \"x\" argument equal to your training data as a data.frame \n",
    "# set the \"y\" argument equal to the training labels vector \n",
    "# set \"tuneGrid\" to your \"k\" dataframe\n",
    "# set the \"trControl\" argument equal to trainControl(method = \"none\") - we will talk more about this argument later\n",
    "model_knn15 <- train(x = data.frame(trainingDat),     \n",
    "                   y = training_labels,\n",
    "                   method = \"knn\", \n",
    "                   tuneGrid = k15,                               \n",
    "                   trControl = trainControl(method = \"none\"))  \n",
    "\n",
    "predictions15 <- predict(object=model_knn15, testingDat)\n",
    "mean(predictions15 == testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi) Which value of $k$ gave the highest accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For various values of $k$, fit a classifier using the training data. Use that classifier to obtain an error rate when predicting on both the training and test sets, for each $k$. How do the training error and test error change with $k$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide this box - looking at the decision boundary for different k values \n",
    "\n",
    "library(caret) \n",
    "\n",
    "mass_length = seq(min(testingDat$scaled_mass), max(testingDat$scaled_mass), by = 0.1)\n",
    "col_length = seq(min(testingDat$scaled_color_score), max(testingDat$scaled_color_score), by = 0.1)\n",
    "\n",
    "# generates the boundaries for your graph\n",
    "lgrid <- expand.grid(scaled_mass=mass_length, \n",
    "                     scaled_color_score = col_length)\n",
    "knnPredGrid <- predict(model_knn5, newdata=lgrid)\n",
    "\n",
    "knnPredGrid = as.numeric(knnPredGrid)\n",
    "\n",
    "# get the points from the test data...\n",
    "testPred <- predict(model_knn5, newdata=testingDat)\n",
    "testPred <- as.numeric(testPred)\n",
    "\n",
    "# this gets the points for the testPred...\n",
    "testingDat$Pred <- testPred\n",
    "\n",
    "probs <- matrix(knnPredGrid, length(mass_length), length(col_length))\n",
    "\n",
    "#ggplot(data=lgrid) + stat_contour(aes(x=scaled_mass, y=scaled_color_score, z=knnPredGrid), \n",
    "#                                  bins=2) +\n",
    "#  geom_point(aes(x=scaled_mass, y=scaled_color_score, colour=as.factor(knnPredGrid))) \n",
    "#  geom_point(data=testingDat, aes(x=testingDat$scaled_mass, y=testingDat$scaled_color_score,\n",
    "#                                  colour=as.factor(testingDat$Pred)),\n",
    "#            size=5, alpha=0.5, shape=1)+\n",
    "#  theme_bw()\n",
    "\n",
    "contour(mass_length, col_length, probs, labels=\"\", xlab=\"\", ylab=\"\", main=\"5-Nearest Neighbor\", axes=F)\n",
    "gd <- expand.grid(x=mass_length, y=col_length)\n",
    "\n",
    "points(gd, pch=\".\", cex=1, col=probs)\n",
    "\n",
    "# add the test points to the graph\n",
    "points(testingDat$scaled_mass, \n",
    "       testingDat$scaled_color_score, col=testingDat$Pred, cex=1, \n",
    "      pch=16)\n",
    "box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnPredGrid <- predict(model_knn1, newdata=lgrid)\n",
    "knnPredGrid = as.numeric(knnPredGrid)\n",
    "\n",
    "# get the points from the test data...\n",
    "testPred <- predict(model_knn1, newdata=testingDat)\n",
    "testPred <- as.numeric(testPred)\n",
    "\n",
    "# this gets the points for the testPred...\n",
    "testingDat$Pred <- testPred\n",
    "\n",
    "probs <- matrix(knnPredGrid, length(mass_length), length(col_length))\n",
    "\n",
    "ggplot(data=lgrid) + stat_contour(aes(x=scaled_mass, y=scaled_color_score, z=knnPredGrid), \n",
    "                                  bins=2) +\n",
    "  geom_point(aes(x=scaled_mass, y=scaled_color_score, colour=as.factor(knnPredGrid))) \n",
    "  geom_point(data=testingDat, aes(x=testingDat$scaled_mass, y=testingDat$scaled_color_score,\n",
    "                                  colour=as.factor(testingDat$Pred)),\n",
    "            size=5, alpha=0.5, shape=1)+\n",
    "  theme_bw()\n",
    "\n",
    "contour(mass_length, col_length, probs, labels=\"\", xlab=\"\", ylab=\"\", main=\"1-Nearest Neighbor\", axes=F)\n",
    "gd <- expand.grid(x=mass_length, y=col_length)\n",
    "\n",
    "points(gd, pch=\".\", cex=1, col=probs)\n",
    "\n",
    "# add the test points to the graph\n",
    "points(testingDat$scaled_mass, \n",
    "       testingDat$scaled_color_score, col=testingDat$Pred, cex=1, \n",
    "      pch=16)\n",
    "box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the k = 1 model using your code above\n",
    "k15 <- data.frame(k = 15)\n",
    "\n",
    "# set the \"x\" argument equal to your training data as a data.frame \n",
    "# set the \"y\" argument equal to the training labels vector \n",
    "# set \"tuneGrid\" to your \"k\" dataframe\n",
    "# set the \"trControl\" argument equal to trainControl(method = \"none\") - we will talk more about this argument later\n",
    "model_knn15 <- train(x = data.frame(trainingDat),     \n",
    "                   y = training_labels,\n",
    "                   method = \"knn\", \n",
    "                   tuneGrid = k15,                               \n",
    "                   trControl = trainControl(method = \"none\"))  \n",
    "\n",
    "knnPredGrid <- predict(model_knn15, newdata=lgrid)\n",
    "knnPredGrid = as.numeric(knnPredGrid)\n",
    "\n",
    "# get the points from the test data...\n",
    "testPred <- predict(model_knn15, newdata=testingDat)\n",
    "testPred <- as.numeric(testPred)\n",
    "\n",
    "# this gets the points for the testPred...\n",
    "testingDat$Pred <- testPred\n",
    "\n",
    "probs <- matrix(knnPredGrid, length(mass_length), length(col_length))\n",
    "\n",
    "ggplot(data=lgrid) + stat_contour(aes(x=scaled_mass, y=scaled_color_score, z=knnPredGrid), \n",
    "                                  bins=2) +\n",
    "  geom_point(aes(x=scaled_mass, y=scaled_color_score, colour=as.factor(knnPredGrid))) \n",
    "  geom_point(data=testingDat, aes(x=testingDat$scaled_mass, y=testingDat$scaled_color_score,\n",
    "                                  colour=as.factor(testingDat$Pred)),\n",
    "            size=5, alpha=0.5, shape=1)+\n",
    "  theme_bw()\n",
    "\n",
    "contour(mass_length, col_length, probs, labels=\"\", xlab=\"\", ylab=\"\", main=\"15-Nearest Neighbor\", axes=F)\n",
    "gd <- expand.grid(x=mass_length, y=col_length)\n",
    "\n",
    "points(gd, pch=\".\", cex=1, col=probs)\n",
    "\n",
    "# add the test points to the graph\n",
    "points(testingDat$scaled_mass, \n",
    "       testingDat$scaled_color_score, col=testingDat$Pred, cex=1, \n",
    "      pch=16)\n",
    "box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German credit example\n",
    "We are going to work a dataset called the Statlog (German Credit Data) Data Set. The data has many attributes, such as \"Status of existing checking account\", \"credit history\" etc for 1000 individuals and classifies people as good or bad credit risks (1 = good, 2 = bad). The dataset can be found [here](http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). \n",
    "\n",
    "Load the dataset \"german.csv\" into the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gDat <- read_csv(\"data/german.csv\")\n",
    "gDat <- gDat %>% \n",
    "  mutate(Default = as.factor(Default)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Partition the data into a training (70%) and testing (30%) set using the `caret` package. Select the \"duration\", \"amount\", and \"default\" variables to include in your datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### R caret \n",
    "set.seed(100)      # set seed so same results can be obtained \n",
    "\n",
    "labels <- gDat %>% \n",
    "  pull(Default)   # pull out a vector rather than having a dataframe\n",
    "\n",
    "# randomly take 70% of the data in the training set proportional to \n",
    "# the number of good and bad credit risk observations in the dataset\n",
    "# list = F denotes that the indices we obtain should form a vector\n",
    "\n",
    "# create index to split based on labels\n",
    "index <- createDataPartition(labels, p = 0.7, list = F)\n",
    "\n",
    "# filtering the dataset into training data based on the random indices above\n",
    "trainingDat <- gDat %>% \n",
    "    select(Duration, Amount) %>% \n",
    "    filter(row_number() %in% index)\n",
    "\n",
    "trainingLabels <-  gDat %>% \n",
    "    select(Default) %>% \n",
    "    filter(row_number() %in% index) %>%\n",
    "    pull()\n",
    "\n",
    "# creating the testing set based on the remaining observations \n",
    "testingDat <- gDat %>% \n",
    "  select(Duration, Amount) %>% \n",
    "  filter(!(row_number() %in% index))\n",
    "\n",
    "testingLabels <- gDat %>% \n",
    "  select(Default) %>% \n",
    "  filter(!(row_number() %in% index)) %>%\n",
    "  pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Perform 10 fold cross validation to select the value of k. What value of k do you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn <- train(x = data.frame(trainingDat),\n",
    "                   y = trainingLabels,\n",
    "                   method = \"knn\", \n",
    "                   trControl = trainControl(method = \"cv\", number = 10),\n",
    "                   preProcess=c(\"center\", \"scale\"))\n",
    "predictions <- predict(model_knn, testingDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3) Evaluate the classification performance by comparing the estimated labels to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix(predictions, testingLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
